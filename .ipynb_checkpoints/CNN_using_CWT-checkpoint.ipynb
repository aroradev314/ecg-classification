{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4405685c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' # removes the warning messages from tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2297bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wfdb\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import pywt\n",
    "import funcs\n",
    "import plot_learning_callback\n",
    "import f1_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9d22501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use gpu\n",
    "\n",
    "gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "for device in gpu_devices:\n",
    "    tf.config.experimental.set_memory_growth(device, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dde15ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"pickled/\"\n",
    "trainReadings = funcs.unpickler(path + 'train_readings.pkl')\n",
    "trainDiagnostic = funcs.unpickler(path + 'train_diagnostic.pkl')\n",
    "validateReadings = funcs.unpickler(path + 'validate_readings.pkl')\n",
    "validateDiagnostic = funcs.unpickler(path + 'validate_diagnostic.pkl')\n",
    "testReadings = funcs.unpickler(path + 'test_readings.pkl')\n",
    "testDiagnostic = funcs.unpickler(path + 'test_diagnostic.pkl')\n",
    "\n",
    "oneHotTrain = funcs.one_hot_encoder(trainDiagnostic, 2)\n",
    "oneHotValidate = funcs.one_hot_encoder(validateDiagnostic, 2)\n",
    "oneHotTest = funcs.one_hot_encoder(testDiagnostic, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe6247f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data is imbalanced so we balance it\n",
    "\n",
    "class_weights = compute_class_weight(\n",
    "                                        class_weight = \"balanced\",\n",
    "                                        classes = np.unique(trainDiagnostic),\n",
    "                                        y = trainDiagnostic                                                    \n",
    "                                    )\n",
    "class_weights = dict(zip(np.unique(trainDiagnostic), class_weights))\n",
    "\n",
    "sigmoidLoss = tf.keras.losses.BinaryCrossentropy() # one output unit\n",
    "softmaxLoss = tf.keras.losses.CategoricalCrossentropy() # two output units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "922c5087",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10977, 1000, 20)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scales = np.linspace(0.1, 4, 20) # scales to use for cwt\n",
    "\n",
    "trainTransform = funcs.make_for_cnn(trainReadings, scales, 'mexh') # apply CWT to all training data\n",
    "validateTransform = funcs.make_for_cnn(validateReadings, scales, 'mexh')\n",
    "testTransform = funcs.make_for_cnn(testReadings, scales, 'mexh')\n",
    "\n",
    "trainTransform.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91920291",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers, metrics, callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab123601",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputShape = trainTransform.shape[1:]\n",
    "kernelSize = 40 #how to determine this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "730fcf1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding these layers keeps the weights the same???????\n",
    "\n",
    "convInputLayer = layers.Conv1D(\n",
    "    filters=64, \n",
    "    kernel_size=kernelSize, \n",
    "    input_shape=trainTransform.shape[1:], \n",
    "    data_format=\"channels_last\", \n",
    "    padding='same')\n",
    "\n",
    "convLayer = layers.Conv1D(\n",
    "    filters=64, \n",
    "    kernel_size=kernelSize, \n",
    "    padding='same')\n",
    "\n",
    "batchNormLayer = layers.BatchNormalization(axis=-1)\n",
    "\n",
    "reLULayer = layers.ReLU()\n",
    "\n",
    "maxPoolLayer = layers.MaxPooling1D(\n",
    "    pool_size=kernelSize * 2, \n",
    "    strides=None, \n",
    "    padding='same') # should the size of the pooling layer be the same as the kernel size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dba3c832",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics=['accuracy', \n",
    "         tf.keras.metrics.Precision(name='precision'), \n",
    "         tf.keras.metrics.Recall(name='recall'),\n",
    "         f1_metric.f1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bddcb950",
   "metadata": {},
   "source": [
    "# Test out alternative architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "589e9272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding dropout layer to prevent overfitting\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "inputDroppedPercent = 50 # during training, 10 percent of the inputs (to the dropout layer) are randomly dropped\n",
    "dropoutLayer = layers.Dropout(inputDroppedPercent / 100)\n",
    "sectionRepeat = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b6e9dc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(layers.Conv1D(\n",
    "    filters=64, \n",
    "    kernel_size=kernelSize, \n",
    "    input_shape=inputShape, \n",
    "    data_format=\"channels_last\", \n",
    "    padding='same'))\n",
    "model.add(layers.BatchNormalization(axis=-1))\n",
    "model.add(layers.Dropout(inputDroppedPercent / 100))\n",
    "model.add(layers.ReLU())\n",
    "model.add(layers.MaxPooling1D(\n",
    "    pool_size=kernelSize * 2, \n",
    "    strides=None, \n",
    "    padding='same'))\n",
    "\n",
    "for i in range(sectionRepeat):\n",
    "    model.add(layers.Conv1D(\n",
    "        filters=64, \n",
    "        kernel_size=kernelSize, \n",
    "        padding='same'))\n",
    "    model.add(layers.BatchNormalization(axis=-1))\n",
    "    model.add(layers.Dropout(inputDroppedPercent / 100))\n",
    "    model.add(layers.ReLU())\n",
    "    model.add(layers.MaxPooling1D(\n",
    "        pool_size=kernelSize * 2, \n",
    "        strides=None, \n",
    "        padding='same'))\n",
    "    \n",
    "\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(1024, activation='relu'))\n",
    "model.add(layers.Dense(1024, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4f97c797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_91 (Conv1D)          (None, 1000, 64)          51264     \n",
      "                                                                 \n",
      " batch_normalization_90 (Bat  (None, 1000, 64)         256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_92 (Dropout)        (None, 1000, 64)          0         \n",
      "                                                                 \n",
      " re_lu_90 (ReLU)             (None, 1000, 64)          0         \n",
      "                                                                 \n",
      " max_pooling1d_45 (MaxPoolin  (None, 13, 64)           0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " conv1d_92 (Conv1D)          (None, 13, 64)            163904    \n",
      "                                                                 \n",
      " batch_normalization_91 (Bat  (None, 13, 64)           256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_93 (Dropout)        (None, 13, 64)            0         \n",
      "                                                                 \n",
      " re_lu_91 (ReLU)             (None, 13, 64)            0         \n",
      "                                                                 \n",
      " max_pooling1d_46 (MaxPoolin  (None, 1, 64)            0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " conv1d_93 (Conv1D)          (None, 1, 64)             163904    \n",
      "                                                                 \n",
      " batch_normalization_92 (Bat  (None, 1, 64)            256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_94 (Dropout)        (None, 1, 64)             0         \n",
      "                                                                 \n",
      " re_lu_92 (ReLU)             (None, 1, 64)             0         \n",
      "                                                                 \n",
      " max_pooling1d_47 (MaxPoolin  (None, 1, 64)            0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " conv1d_94 (Conv1D)          (None, 1, 64)             163904    \n",
      "                                                                 \n",
      " batch_normalization_93 (Bat  (None, 1, 64)            256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_95 (Dropout)        (None, 1, 64)             0         \n",
      "                                                                 \n",
      " re_lu_93 (ReLU)             (None, 1, 64)             0         \n",
      "                                                                 \n",
      " max_pooling1d_48 (MaxPoolin  (None, 1, 64)            0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " flatten_9 (Flatten)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 1024)              66560     \n",
      "                                                                 \n",
      " dense_24 (Dense)            (None, 1024)              1049600   \n",
      "                                                                 \n",
      " dense_25 (Dense)            (None, 1)                 1025      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,661,185\n",
      "Trainable params: 1,660,673\n",
      "Non-trainable params: 512\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "61dde562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a checkpoint for the best val\n",
    "\n",
    "currentDirectory = os.getcwd()\n",
    "f1FileName = 'bestf1.hdf5' # model weights are stored in hdf5 file\n",
    "f1File = os.path.join(currentDirectory, 'ckpt', f1FileName)\n",
    "\n",
    "f1Checkpoint = callbacks.ModelCheckpoint(filepath=f1File, \n",
    "                                       save_weights_only=True, \n",
    "                                       monitor='f1',\n",
    "                                       mode='max', #save when the f1 score is the highest\n",
    "                                       save_best_only=True, \n",
    "                                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "444899c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam', loss=sigmoidLoss, \n",
    " metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318f86bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    trainTransform, trainDiagnostic, \n",
    "    epochs=10,\n",
    "    validation_data=(validateTransform, validateDiagnostic), \n",
    "    class_weight=class_weights,\n",
    "    callbacks=[plot_learning_callback.PlotLearning(), f1Checkpoint],\n",
    "    batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "5d712661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_21\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_24 (Conv1D)          (None, 1000, 64)          51264     \n",
      "                                                                 \n",
      " batch_normalization_11 (Bat  multiple                 256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        multiple                  0         \n",
      "                                                                 \n",
      " re_lu_11 (ReLU)             multiple                  0         \n",
      "                                                                 \n",
      " max_pooling1d_9 (MaxPooling  multiple                 0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " conv1d_26 (Conv1D)          (None, 13, 64)            163904    \n",
      "                                                                 \n",
      " batch_normalization_12 (Bat  (None, 13, 64)           256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 13, 64)            0         \n",
      "                                                                 \n",
      " re_lu_12 (ReLU)             (None, 13, 64)            0         \n",
      "                                                                 \n",
      " max_pooling1d_10 (MaxPoolin  (None, 1, 64)            0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " flatten_12 (Flatten)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_36 (Dense)            (None, 1024)              66560     \n",
      "                                                                 \n",
      " dense_37 (Dense)            (None, 1024)              1049600   \n",
      "                                                                 \n",
      " dense_38 (Dense)            (None, 1)                 1025      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,332,865\n",
      "Trainable params: 1,332,609\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4322ae1e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "funcs.graph_losses(history, \"Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866d2a32",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "funcs.graph_losses(history, \"Accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d83407d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 3))\n",
    "plt.plot(history.history['f1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea5b340",
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa28c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "funcs.evaluate_model(model.predict(testTransform), testDiagnostic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241fa2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "bestf1Model = tf.keras.models.clone_model(modelv2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf5a2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "bestf1Model.compile(\n",
    "    optimizer='adam', loss=softmaxLoss, \n",
    " metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32aaa69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bestf1Model.load_weights('ckpt/bestf1.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2a95e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "bestf1Model.evaluate(testTransform, oneHotTest)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow 2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
